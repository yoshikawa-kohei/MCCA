}
PI1<-(1/(1+apply(1/PI,1,sum)))
PIJ<-cbind(PI1,PI1/PI)
for(x in 1:n0){
C0[x]<-sample(1:J0,1,prob=c(PIJ[x,]))
}
data<-cbind(C0,datax)
while((sum(data[,1]==1)< C_n) | (sum(data[,1]==2)< C_n) | (sum(data[,1]==3)< C_n) | (sum(data[,1]==4)< C_n) | (sum(data[,1]==5)< C_n)){
setdata<-setdata+1
set.seed(r+100*(setdata))
datax<-cbind(rep(1,n0),mvrnorm(n0,numeric(p0),covd))
BXm<-datax%*%beta00
C0<-numeric(n0)
Pi<-exp(BXm)
PI<-Pi[,1]
for(k in 2:J10){
PI<-cbind(PI,apply(Pi[,1:k],1,prod))
}
PI1<-(1/(1+apply(1/PI,1,sum)))
PIJ<-cbind(PI1,PI1/PI)
for(x in 1:n0){
C0[x]<-sample(1:J0,1,prob=c(PIJ[x,]))
}
data<-cbind(C0,datax)
}
n<-nrow(data) #サンプル数
p<-ncol(data)-1 #変数の数
J<-length(unique(data[,1])) #クラス数
J1<-J-1
beta<-c(rep(0,(J-1)*p))
W<-array(0,dim=c(J1*p,J1*p,p))
Z<-array(0,dim=c(J1*p,J1*p,p))
#Yi*
Y<-function(i){
y<-numeric(J1)
for(k in 1:J1){
if(k==data[i,1]){
y[k]<- 1
}}
y
}
#Xi*
X<-function(i){
x<-matrix(numeric((J-1)*(J-1)*p),J-1,p*(J-1))
for(k in 1:J-1){
a<-k*p
x[k,(a-p+1):a]<-as.numeric(data[i,2:(p+1)])
}
x
}
#Y(i)%*%X(i)
YX<-array(NA_real_,dim=c(1,J1*p,n))
for(i in 1:n){
YX[,,i]<-Y(i)%*%X(i)
}
#Ir
I<-function(k){
Ir<-numeric(J1)
Ir[k]<-1
Ir
}
#Ir(k)*X(i)
IX<-array(NA_real_,dim=c(J1*p,J1,n))
for(i in 1:n){
for(k in 1:J1){
IX[,k,i]<-I(k)%*%X(i)
}}
#IXbetaIX
IXbetaIX<-function(i){
ixbetaix<-numeric(p*J1)
for(k in 1:J1){
ixbetaix<-as.numeric(exp(IX[,k,i]%*%beta))*as.numeric(IX[,k,i])+ixbetaix
}
ixbetaix
}
#IXbeta
IXbeta<-function(i){
ixbeta<-0
for(k in 1:J1){
ixbeta<-as.numeric(exp(IX[,k,i]%*%beta))+ixbeta
}
ixbeta
}
#IXbeta3
IXbeta3<-function(i){
ixbeta3<-matrix(numeric((p*J1)^2),(p*J1),(p*J1))
for(k in 1:J1){
ixbeta3<-as.numeric(exp(IX[,k,i]%*%beta))*as.numeric(IX[,k,i])%o%as.numeric(IX[,k,i])+ixbeta3
}
ixbeta3
}
###一階微分###
D<-function(beta){
Dn<-numeric((J1)*p)
for(i in 1:n){
dn<-Y(i)%*%X(i)-(IXbetaIX(i))/(1+IXbeta(i))
Dn<-dn+Dn
}
t(Dn)
}
###二階微分###
DD<-function(beta){
DDn<-matrix(numeric(((J-1)*p)^2),(J-1)*p,(J-1)*p)
for(i in 1:n){
ddn1<-(IXbetaIX(i))%*%t(IXbetaIX(i))
ddn2<-(IXbeta3(i))*(1+IXbeta(i))
ddn<-(ddn1-ddn2)/(1+IXbeta(i))^2
DDn<-DDn+ddn
}
DDn
}
###Bj,m###
Bjm<-function(k){
Bjm<-matrix(0,p,p*J1)
if(k < J){
Bjm[,((k-1)*p+1):(p*k)]<-diag(p)
}
Bjm
}
###LL###
LL<-function(beta){
LL<-0
for(i in 1:n){
LL<-LL+YX[,,i]%*%beta-log(1+sum(exp(X(i)%*%beta)))
}
LL
}
#####Rigde#####
rho=1 #学習率
ip=0.1 #収束条件である更新量の閾値
N=100 #繰り返し回数の上限
###############
beta<-c(rep(0,(J-1)*p)) #パラメータ初期値
a<-1
UDn<-rep(1,(J-1)*p)
IP<-rep(ip,(J-1)*p)
A<-1
while(A>0){
a<-a+1
UDn<-rho*solve(DD(beta)+1e-5*diag(p*(J-1)))%*%(D(beta))
beta<-beta-UDn
A<-sum(abs(UDn)> IP)
if (a >= N){
break;
}
}
betaRim<-matrix(c(beta,rep(0,p)),p,J)
zRim<-matrix(0,p,J1)
for(k in 1:J1){
zRim[,k]<-betaRim[,k]-betaRim[,(k+1)]
}
######lambda######
Test_Fun<-function(lambda){
#Yi*
Y<-function(i){
y<-numeric(J1)
for(k in 1:J1){
if(k==data[i,1]){
y[k]<- 1
}}
y
}
#Xi*
X<-function(i){
x<-matrix(numeric((J-1)*(J-1)*p),J-1,p*(J-1))
for(k in 1:J-1){
a<-k*p
x[k,(a-p+1):a]<-as.numeric(data[i,2:(p+1)])
}
x
}
#Ir
I<-function(k){
Ir<-numeric(J1)
Ir[k]<-1
Ir
}
#IXbetaIX
IXbetaIX<-function(i){
ixbetaix<-numeric(p*J1)
for(k in 1:J1){
ixbetaix<-as.numeric(exp(IX[,k,i]%*%beta))*as.numeric(IX[,k,i])+ixbetaix
}
ixbetaix
}
#IXbeta
IXbeta<-function(i){
ixbeta<-0
for(k in 1:J1){
ixbeta<-as.numeric(exp(IX[,k,i]%*%beta))+ixbeta
}
ixbeta
}
#IXbeta3
IXbeta3<-function(i){
ixbeta3<-matrix(numeric((p*J1)^2),(p*J1),(p*J1))
for(k in 1:J1){
ixbeta3<-as.numeric(exp(IX[,k,i]%*%beta))*as.numeric(IX[,k,i])%o%as.numeric(IX[,k,i])+ixbeta3
}
ixbeta3
}
###一階微分###
D<-function(beta){
Dn<-numeric((J1)*p)
for(i in 1:n){
dn<-Y(i)%*%X(i)-(IXbetaIX(i))/(1+IXbeta(i))
Dn<-dn+Dn
}
t(Dn)
}
###二階微分###
DD<-function(beta){
DDn<-matrix(numeric(((J-1)*p)^2),(J-1)*p,(J-1)*p)
for(i in 1:n){
ddn1<-(IXbetaIX(i))%*%t(IXbetaIX(i))
ddn2<-(IXbeta3(i))*(1+IXbeta(i))
ddn<-(ddn1-ddn2)/(1+IXbeta(i))^2
DDn<-DDn+ddn
}
DDn
}
###Bj,m###
Bjm<-function(k){
Bjm<-matrix(0,p,p*J1)
if(k < J){
Bjm[,((k-1)*p+1):(p*k)]<-diag(p)
}
Bjm
}
###LL###
LL<-function(beta){
LL<-0
for(i in 1:n){
LL<-LL+YX[,,i]%*%beta-log(1+sum(exp(X(i)%*%beta)))
}
LL
}
###ADMM###
rho<-1#ADMMパラメータ
ip<-0.1 #収束条件である更新量の閾値
N<-100 #繰り返し回数の上限
ad<-0#ADMM収束までの回数
END<-0#収束条件
LLF<-0#対数尤度
while(END==0){
ad<-ad+1
####beta####
betak<-beta
nt<-0#ニュートン法の収束回数
UDn<-rep(1,(J-1)*p)
IP<-rep(ip,(J-1)*p)
end<-1#ニュートン法収束条件
while(end>0){
nt<-nt+1
bassoku1D<-numeric(p*J1)
bassoku2D<-numeric(p*J1)
bassoku2DD<-matrix(0,p*J1,p*J1)
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(Gfus[k1,k2]==1){
bassoku1D<-t(Bjm(k1)-Bjm(k2))%*%W[k1,k2,]+bassoku1D
bassoku2D<-(t(Bjm(k1))%*%Bjm(k1)+t(Bjm(k2))%*%Bjm(k2)-t(Bjm(k1))%*%Bjm(k2)-t(Bjm(k2))%*%Bjm(k1))%*%beta+t(-Bjm(k1)+Bjm(k2))%*%Z[k1,k2,]+bassoku2D
bassoku2DD<-(t(Bjm(k1))%*%Bjm(k1)+t(Bjm(k2))%*%Bjm(k2)-t(Bjm(k1))%*%Bjm(k2)-t(Bjm(k2))%*%Bjm(k1))+bassoku2DD
}}}
UDn<-solve(-DD(beta)+rho*bassoku2DD)%*%(-D(beta)+bassoku1D+rho*bassoku2D)
beta<-beta-UDn
end<-sum(abs(UDn)> IP)
if (nt >= N){
break;
}
}#ニュートン法
####beta####
####Z####
beta_m<-cbind(matrix(beta,p,J1),0)
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(Gfus[k1,k2]==1){
Z_in<-1-(lambda/(rho*sqrt(sum((beta_m[,k1]-beta_m[,k2]+(1/rho)*W[k1,k2,])^2))))
if(Z_in < 0){ Z_in<-0 }
Z[k1,k2,]<-(beta_m[,k1]-beta_m[,k2]+(1/rho)*W[k1,k2,])*Z_in
}
}}
####Z#####
####W####
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(Gfus[k1,k2]==1){
W[k1,k2,]<-W[k1,k2,]+rho*(beta_m[,k1]-beta_m[,k2]-Z[k1,k2,])
}
}}
####W####
LLFk<-LLF
LLF<-LL(beta)
END<-as.numeric(abs((LLF-LLFk)/LLFk)<0.001)
if (ad >= N){
break;
}}
CLASS<-c(1,numeric(J1))
cl_k<-1
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(sum(abs(Z[k1,k2,]))==0){
CLASS[k2]<-cl_k
}else{
cl_k<-cl_k+1
CLASS[k2]<-cl_k
}}}
betaK1<-numeric(J1*p)
for(k in 1:max(CLASS)){
for(K in 1:J1){
if((CLASS[K]==k)&&(CLASS[K]!=max(CLASS))){
class<-(CLASS==k)[1:J1]
betaK1[(p*K-p+1):(p*K)]<-beta_m[,1:J1]%*%class/sum(class)
}else if(CLASS[K]==max(CLASS)){
betaK1[(p*K-p+1):(p*K)]<-numeric(p)}
}}
Zm<-matrix(0,p,J1)
for(k in 1:J1){
Zm[,k]<-Z[k,(k+1),]
}
df<-sum(sqrt(apply(Zm^2,2,sum))>0)+(p-1)*sum(sqrt(apply(Zm^2,2,sum))/sqrt(apply(zRim^2,2,sum)))
list(Score = (2*LL(betaK1)-df*log(n)),Pred = 0)
}
BO<-try(BayesianOptimization(Test_Fun, bounds=list(lambda=c(0.1,100)),init_points=50, n_iter=1, acq='ei', kappa=2.576,eps=0.0, verbose=TRUE))
tryCatch(
{ lambda<-BO$Best_Par }
, error = function(e) { lambda<-0.1}
)
##########lambda##########
###ADMM###
rho<-1#ADMMパラメータ
ip<-0.1 #収束条件である更新量の閾値
N<-100 #繰り返し回数の上限
#lambda<-BO$Best_Par
##########
ad<-0#ADMM収束までの回数
END<-0#収束条件
LLF<-0#対数尤度
while(END==0){
ad<-ad+1
####beta####
betak<-beta
nt<-0#ニュートン法の収束回数
UDn<-rep(1,(J-1)*p)
IP<-rep(ip,(J-1)*p)
end<-1#ニュートン法収束条件
while(end>0){
nt<-nt+1
bassoku1D<-numeric(p*J1)
bassoku2D<-numeric(p*J1)
bassoku2DD<-matrix(0,p*J1,p*J1)
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(Gfus[k1,k2]==1){
bassoku1D<-t(Bjm(k1)-Bjm(k2))%*%W[k1,k2,]+bassoku1D
bassoku2D<-(t(Bjm(k1))%*%Bjm(k1)+t(Bjm(k2))%*%Bjm(k2)-t(Bjm(k1))%*%Bjm(k2)-t(Bjm(k2))%*%Bjm(k1))%*%beta+t(-Bjm(k1)+Bjm(k2))%*%Z[k1,k2,]+bassoku2D
bassoku2DD<-(t(Bjm(k1))%*%Bjm(k1)+t(Bjm(k2))%*%Bjm(k2)-t(Bjm(k1))%*%Bjm(k2)-t(Bjm(k2))%*%Bjm(k1))+bassoku2DD
}}}
UDn<-solve(-DD(beta)+rho*bassoku2DD)%*%(-D(beta)+bassoku1D+rho*bassoku2D)
beta<-beta-UDn
end<-sum(abs(UDn)> IP)
if (nt >= N){
break;
}
}#ニュートン法
####beta####
####Z####
beta_m<-cbind(matrix(beta,p,J1),0)
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(Gfus[k1,k2]==1){
Z_in<-1-(lambda/(rho*sqrt(sum((beta_m[,k1]-beta_m[,k2]+(1/rho)*W[k1,k2,])^2))))
if(Z_in < 0){ Z_in<-0 }
Z[k1,k2,]<-(beta_m[,k1]-beta_m[,k2]+(1/rho)*W[k1,k2,])*Z_in
}
}}
####Z#####
####W####
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(Gfus[k1,k2]==1){
W[k1,k2,]<-W[k1,k2,]+rho*(beta_m[,k1]-beta_m[,k2]-Z[k1,k2,])
}
}}
####W####
LLFk<-LLF
LLF<-LL(beta)
END<-as.numeric(abs((LLF-LLFk)/LLFk)<0.001)
if (ad >= N){
break;
}
}
###β推定値###
beta_m<-cbind(matrix(beta,p,J1),0)
CLASS<-c(1,numeric(J1))
cl_k<-1
for(k1 in 1:J1){
for(k2 in (k1+1):J){
if(sum(abs(Z[k1,k2,]))==0){
CLASS[k2]<-cl_k
}else{
cl_k<-cl_k+1
CLASS[k2]<-cl_k
}}}
betaK1<-numeric(J1*p)
for(k in 1:max(CLASS)){
for(K in 1:J1){
if((CLASS[K]==k)&&(CLASS[K]!=max(CLASS))){
class<-(CLASS==k)[1:J1]
betaK1[(p*K-p+1):(p*K)]<-beta_m[,1:J1]%*%class/sum(class)
}else if(CLASS[K]==max(CLASS)){
betaK1[(p*K-p+1):(p*K)]<-numeric(p)}
}}
######KL計算#############
KL<-0
for(kl in 1:100){
datax<-cbind(rep(1,n0),mvrnorm(n0,numeric(p0),covd))
BXm<-datax%*%beta00
Pi<-exp(BXm)
PI<-Pi[,1]
for(k in 2:J1){
PI<-cbind(PI,apply(Pi[,1:k],1,prod))
}
PI1<-(1/(1+apply(1/PI,1,sum)))
PIJtr<-cbind(PI1,PI1/PI)#配分確率真
beta_mk<-cbind(matrix(betaK1,p,J1),0)
ZXm<-datax%*%beta_mk
PIJte<-exp(ZXm)/apply(exp(ZXm),1,sum)
#配分確率test
KL<-KL+sum(PIJte*log(PIJte/PIJtr))/100
}
########################
if(sum(CLASS==c(1,1,2,3,4))==5){
m<-m+1
}
print(r)
print(m)
CLASSn[r]<-max(CLASS)#クラス数
CLASS_J[r,]<-CLASS
KL100[r]<-KL
}
time<-proc.time()-t
sink(file = output.filename, append=T)
cat("正当数 m : ", m, "\n")
cat("KL平均 mean(KL100) : ", mean(KL100), "\n")
cat("KL標準偏差 sd(KL100) : ", sd(KL100), "\n")
cat("クラス数平均 mean(CLASSn) : ", mean(CLASSn), "\n")
cat("クラス数標準偏差 sd(CLASSn) : ", sd(CLASSn), "\n")
sink()
Sys.setenv("http_proxy"="")
Sys.setenv("https_proxy"="")
Sys.setenv("ftp_proxy"="")
install.packages("rrpack")
install.packages(c("backports", "broom", "Cairo", "callr", "carData", "caret", "classInt", "covr", "crosstalk", "dbplyr", "devtools", "dplyr", "DT", "ellipsis", "fBasics", "fda", "fMultivar", "foreach", "forecast", "fs", "gamm4", "gdtools", "gglasso", "git2r", "glmnet", "glue", "gmp", "gss", "gstat", "gtools", "GWmodel", "igraph", "intervals", "isoband", "KernSmooth", "leafem", "lhs", "lifecycle", "lme4", "lubridate", "maptools", "mapview", "mclust", "mnormt", "ModelMetrics", "modelr", "nlme", "nloptr", "pillar", "pkgbuild", "pkgmaker", "plotrix", "pROC", "ps", "purrr", "quantmod", "raster", "Rcpp", "RcppArmadillo", "RCurl", "recipes", "reshape2", "rex", "rgeos", "rgl", "rlang", "RLRsim", "robustbase", "roxygen2", "rTensor", "scales", "sf", "shiny", "sn", "sp", "spatstat", "spData", "systemfonts", "tibble", "tidyr", "tidyselect", "tinytex", "topicmodels", "units", "usethis", "vctrs", "withr", "xfun", "xml2", "zoo"))
install.packages(c("boot", "class", "KernSmooth", "lattice", "MASS", "nlme", "nnet", "spatial", "survival"), lib="/usr/local/Cellar/r/3.6.2/lib/R/library")
install.packages(c("igraph", "nlme"))
install.packages("nlme", lib="/usr/local/Cellar/r/3.6.2/lib/R/library")
library(LaplacesDemon)
library(tidyverse)
library(tictoc)
library(imager)
library(MASS)
library(rTensor)
library(progress)
library(abind)
library(ggplot2)
set.seed(1)
rnom(1)
rnorm
rnorm(n=100)
hist(rnorm(n=100))
hist
help("hist")
hist(rnorm(n=1000))
setwd("~/Documents/MCCA/MCCA")
source("R/cca1d.R")
# Import some libraries and our source codes ------------------------------
source("R/cca1d.R")
source("R/pca1d.R")
source("R/TensorCCA.R")
source("R/generate_simulation_data.R")
library(LaplacesDemon)
library(MASS)
library(rTensor)
library(progress)
library(abind)
# set a seed if you need.
set.seed(1)
# Generate a dataset -------------------------------------------------------
# settings for generating the synthentic data
n <- 100 # sample size
p1 <- 50 # dimension of mode-1
p2 <- 50 # dimension of mode-2
num.group <- 3 # the number of groups
# generate dataset for MCCA
data <- generate.sim.data(n, p1, p2, num.group)
# specify the dimensions of latent space
rank1 <- 10
rank2 <- 10
fit.mcca <- mcca(data, ranks=c(rank1,rank2))
# the samples in latent space
data.core <- fit.mcca$core
# the approximated samples
data.est <- fit.mcca$est
# the reconstructed samples : fit.mcca$est + fit.mcca$tnsr.mean
data.reconst <- fit.mcca$tnsr.reconst
# projection matrices
proj <- fit.mcca$V
# latent covariance matrices
cov <- fit.mcca$Lambda
# reconstruction error rate (RER)
rer <- fit.mcca$reconst.rate
